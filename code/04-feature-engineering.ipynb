{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from window_ops.rolling import (\n",
    "    seasonal_rolling_max,\n",
    "    seasonal_rolling_mean,\n",
    "    seasonal_rolling_min,\n",
    "    seasonal_rolling_std,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 - Feature Engineering\n",
    "\n",
    "A standard regression model has no explicit understanding of time and so we need to engineer good features to embed the temporal aspect of the problem. We will do time delay embedding to add recent and seasonal information to the dataset.\n",
    "\n",
    "We read in the versions of the train, val and test portions of the data where we imputed the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = Path('../data/preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the train, val and test data sets where missing values are imputed\n",
    "train_df = pd.read_parquet(preprocessed/'selected_blocks_train_missing_imputed.parquet')\n",
    "val_df = pd.read_parquet(preprocessed/'selected_blocks_val_missing_imputed.parquet')\n",
    "test_df = pd.read_parquet(preprocessed/'selected_blocks_test_missing_imputed.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features we will be creating will require a single joined-up dataset with continuous time. We join them up while tagging each portion so that we don't lose track:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['type'] = 'train'\n",
    "val_df['type'] = 'val'\n",
    "test_df['type'] = 'test'\n",
    "full_df = pd.concat([train_df, val_df, test_df]).sort_values(['LCLid', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the appropriate 32 bit data type for a column\n",
    "def get_32_bit_dtype(col):\n",
    "    dtype = col.dtype\n",
    "    if dtype.name.startswith('float'):\n",
    "        ret_dtype = 'float32'\n",
    "    elif dtype.name.startswith('int'):\n",
    "        ret_dtype = 'int32'\n",
    "    else:\n",
    "        ret_dtype = None\n",
    "    return ret_dtype\n",
    "\n",
    "# Add lags, setting the data type of the lag columns to 32 bit if we want to save space\n",
    "# performs the lags for each LCLId separately\n",
    "def add_lags(df, lags, column, ts_id, use_32_bit=False):\n",
    "    _32_bit_dtype = get_32_bit_dtype(df[column])\n",
    "    if use_32_bit and _32_bit_dtype is not None:\n",
    "        col_dict = {f'{column}_lag_{l}': df.groupby([ts_id])[column].shift(l).astype(_32_bit_dtype) for l in lags}\n",
    "    else:\n",
    "        col_dict = {f'{column}_lag_{l}': df.groupby([ts_id])[column].shift(l) for l in lags}\n",
    "    df = df.assign(**col_dict)\n",
    "    added_features = list(col_dict.keys())\n",
    "    return df, added_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the first 5 lags, the same 5 lags from the previous day, and the same 5 lags from the previous week in order to capture daily and weekly seasonality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = (\n",
    "    (np.arange(5) + 1).tolist()\n",
    "    + (np.arange(5) + 46).tolist()\n",
    "    + (np.arange(5) + (48 * 7) - 2).tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, added_features = add_lags(full_df, lags, 'energy_consumption', 'LCLid', True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add a rolling window aggregation (using the mean average and standard deviation) of with window sizes of 3, 6, 12 and 18 to connect the present data point to the recent past while smoothing out fluctuations and short-term irregularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_features(df, rolls, column, agg_funcs=['mean', 'std'], ts_id=None, n_shift=1, use_32_bit=False):\n",
    "    _32_bit_dtype = get_32_bit_dtype(df[column])\n",
    "    rolling_df = pd.concat([df.groupby(ts_id)[column]\n",
    "                            .shift(n_shift).rolling(l)\n",
    "                            .agg({f'{column}_rolling_{l}_{agg}': agg for agg in agg_funcs}) \n",
    "                            for l in rolls], axis=1)\n",
    "    df = df.assign(**rolling_df.to_dict('list'))\n",
    "    added_features = rolling_df.columns.tolist()\n",
    "    if use_32_bit and _32_bit_dtype is not None:\n",
    "        df[added_features] = df[added_features].astype(_32_bit_dtype)\n",
    "    return df, added_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, added_features = add_rolling_features(\n",
    "    full_df,\n",
    "    rolls=[3, 6, 12, 48],\n",
    "    column='energy_consumption',\n",
    "    agg_funcs=['mean', 'std'],\n",
    "    ts_id='LCLid',\n",
    "    use_32_bit=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seasonal rolling window aggregation takes a seasonal window, skipping a constant number of timesteps between each item in the window. Since this is not easy to do efficiently with Pandas, we use an implementation from [github.com/jmoralez/window_ops/](https://github.com/jmoralez/window_ops/) that uses NumPy and Numba to make doing this fast and efficient.\n",
    "\n",
    "We add periods of 48 (one day) and 28*7 (one week), each with a window of size 3, aggregating on mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEASONAL_ROLLING_MAP = {\n",
    "    'mean': seasonal_rolling_mean,\n",
    "    'min': seasonal_rolling_min,\n",
    "    'max': seasonal_rolling_max,\n",
    "    'std': seasonal_rolling_std,\n",
    "}\n",
    "\n",
    "def add_seasonal_rolling_features(df, seasonal_periods, rolls, column, agg_funcs, ts_id=None, n_shift=1, use_32_bit=False):\n",
    "    _32_bit_dtype = get_32_bit_dtype(df[column])\n",
    "    agg_funcs = {agg: SEASONAL_ROLLING_MAP[agg] for agg in agg_funcs}\n",
    "    added_features = []\n",
    "    for sp in seasonal_periods:\n",
    "        if use_32_bit and _32_bit_dtype is not None:\n",
    "            col_dict = {\n",
    "                f'{column}_{sp}_seasonal_rolling_{l}_{name}': df.groupby(ts_id)[\n",
    "                    column\n",
    "                ]\n",
    "                .transform(\n",
    "                    lambda x: agg(\n",
    "                        x.shift(n_shift * sp).values,\n",
    "                        season_length=sp,\n",
    "                        window_size=l,\n",
    "                    )\n",
    "                )\n",
    "                .astype(_32_bit_dtype)\n",
    "                for (name, agg) in agg_funcs.items()\n",
    "                for l in rolls\n",
    "            }\n",
    "        else:\n",
    "            col_dict = {\n",
    "                f'{column}_{sp}_seasonal_rolling_{l}_{name}': df.groupby(ts_id)[\n",
    "                    column\n",
    "                ].transform(\n",
    "                    lambda x: agg(\n",
    "                        x.shift(n_shift * sp).values,\n",
    "                        season_length=sp,\n",
    "                        window_size=l,\n",
    "                    )\n",
    "                )\n",
    "                for (name, agg) in agg_funcs.items()\n",
    "                for l in rolls\n",
    "            }\n",
    "        df = df.assign(**col_dict)\n",
    "        added_features += list(col_dict.keys())\n",
    "    return df, added_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, added_features = add_seasonal_rolling_features(\n",
    "    full_df,\n",
    "    rolls=[3],\n",
    "    seasonal_periods=[48, 48 * 7],\n",
    "    column='energy_consumption',\n",
    "    agg_funcs=['mean', 'std'],\n",
    "    ts_id='LCLid',\n",
    "    use_32_bit=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentially Weighted Moving Averages can be thought of as an average of the entire history of the time series, but where exponentially decaying weights cause more recent points in time to have more weight in the average. These can be defined in terms of the decay parameter alpha, or the span (the number of periods that it takes for the decayed weights to approach zero), where alpha = 2 / (1 + span).\n",
    "\n",
    "Here, we add EWMA features with spans of 60 days, 1 week, and 1 day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ewma(df, column, alphas=[0.5], spans=None, ts_id=None, n_shift=1, use_32_bit=False):\n",
    "    # determine whether to use spans or alphas in ewm calculations\n",
    "    if spans is not None:\n",
    "        use_spans = True\n",
    "    _32_bit_dtype = get_32_bit_dtype(df[column])\n",
    "    if use_32_bit and _32_bit_dtype is not None:\n",
    "        col_dict = {\n",
    "            f\"{column}_ewma_{'span' if use_spans else 'alpha'}_{param}\": df.groupby(\n",
    "                [ts_id]\n",
    "            )[column]\n",
    "            .shift(n_shift)\n",
    "            .ewm(\n",
    "                alpha=None if use_spans else param,\n",
    "                span=param if use_spans else None,\n",
    "                adjust=False,\n",
    "            )\n",
    "            .mean()\n",
    "            .astype(_32_bit_dtype)\n",
    "            for param in (spans if use_spans else alphas)\n",
    "        }\n",
    "    else:\n",
    "        col_dict = {\n",
    "            f\"{column}_ewma_{'span' if use_spans else 'alpha'}_{param}\": df.groupby(\n",
    "                [ts_id]\n",
    "            )[column]\n",
    "            .shift(n_shift)\n",
    "            .ewm(\n",
    "                alpha=None if use_spans else param,\n",
    "                span=param if use_spans else None,\n",
    "                adjust=False,\n",
    "            )\n",
    "            .mean()\n",
    "            for param in (spans if use_spans else alphas)\n",
    "        }\n",
    "    df = df.assign(**col_dict)\n",
    "    return df, list(col_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, added_features = add_ewma(\n",
    "    full_df,\n",
    "    spans=[48 * 60, 48 * 7, 48],\n",
    "    column='energy_consumption',\n",
    "    ts_id='LCLid',\n",
    "    use_32_bit=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through temporal embedding, we can embed time features which the ML model can leverage.\n",
    "\n",
    "We can add calendar features such as the month, quarter, day of year, hour, minute and so on as categorical variables. It only makes sense to add calendar features which are temporally higher than the frequency of the time series.\n",
    "\n",
    "We can also add elapsed-time features which capture the passage of time in an ML model. These columns increate monotonically as time increases. Here, we add a timestamp column to indicate the elapsed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_features(df, date_column_name, add_elapsed, prefix=None, drop=True, use_32_bit=False):\n",
    "    date_column = df[date_column_name]\n",
    "    prefix = (re.sub('[Dd]ate$', '', date_column_name) if prefix is None else prefix) + '_'\n",
    "    # because we have 30min-frequency data, we can add the following features:\n",
    "    attr = ['Month',\n",
    "    'Quarter',\n",
    "    'Is_quarter_end',\n",
    "    'Is_quarter_start',\n",
    "    'Is_year_end',\n",
    "    'Is_year_start',\n",
    "    'Is_month_start',\n",
    "    'WeekDay',\n",
    "    'Dayofweek',\n",
    "    'Dayofyear',\n",
    "    'Hour',\n",
    "    'Minute']\n",
    "    _32_bit_dtype = 'int32'\n",
    "    added_features = []\n",
    "    for n in attr:\n",
    "        if n == 'Week':\n",
    "            continue\n",
    "        df[prefix + n] = (\n",
    "            getattr(date_column.dt, n.lower()).astype(_32_bit_dtype)\n",
    "            if use_32_bit\n",
    "            else getattr(date_column.dt, n.lower())\n",
    "        )\n",
    "        added_features.append(prefix + n)\n",
    "    # Pandas removed `dt.week` in v1.1.10\n",
    "    if 'Week' in attr:\n",
    "        week = (\n",
    "            date_column.dt.isocalendar().week\n",
    "            if hasattr(date_column.dt, 'isocalendar')\n",
    "            else date_column.dt.week\n",
    "        )\n",
    "        df.insert(\n",
    "            3, prefix + 'Week', week.astype(_32_bit_dtype) \n",
    "            if use_32_bit \n",
    "            else week\n",
    "        )\n",
    "        added_features.append(prefix + 'Week')\n",
    "    if add_elapsed:\n",
    "        mask = ~date_column.isna()\n",
    "        df[prefix + 'Elapsed'] = np.where(\n",
    "            mask, date_column.values.astype(np.int64) // 10**9, None\n",
    "        )\n",
    "        if use_32_bit:\n",
    "            if df[prefix + 'Elapsed'].isnull().sum() == 0:\n",
    "                df[prefix + 'Elapsed'] = df[prefix + 'Elapsed'].astype('int32')\n",
    "            else:\n",
    "                df[prefix + 'Elapsed'] = df[prefix + 'Elapsed'].astype('float32')\n",
    "        added_features.append(prefix + 'Elapsed')\n",
    "    if drop:\n",
    "        df.drop(date_column_name, axis=1, inplace=True)\n",
    "    return df, added_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, added_features = add_temporal_features(\n",
    "    full_df,\n",
    "    date_column_name='timestamp',\n",
    "    add_elapsed=True,\n",
    "    drop=False,\n",
    "    use_32_bit=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing categorical calendar features as their continuous sine and cosine Fourier components can be advantageous depending on the kind of ML model being used.\n",
    "\n",
    "Here, we encode the `timestamp_Month`, `timestamp_Hour` and `timestamp_minute` columns as their Fourier representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate n Fourier terms, given the seasonal cycle and max cycle:\n",
    "def calculate_fourier_terms(seasonal_cycle, max_cycle, n_fourier_terms):\n",
    "    sin_X = np.empty((len(seasonal_cycle), n_fourier_terms), dtype='float64')\n",
    "    cos_X = np.empty((len(seasonal_cycle), n_fourier_terms), dtype='float64')\n",
    "    for i in range(1, n_fourier_terms + 1):\n",
    "        sin_X[:, i - 1] = np.sin((2 * np.pi * seasonal_cycle * i) / max_cycle)\n",
    "        cos_X[:, i - 1] = np.cos((2 * np.pi * seasonal_cycle * i) / max_cycle)\n",
    "    return np.hstack([sin_X, cos_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the Fourier features for an individual column\n",
    "def add_fourier_features(df, column_to_encode, max_value=None, Optional=None, n_fourier_terms=1, use_32_bit=False):\n",
    "    if max_value is None:\n",
    "        max_value = df[column_to_encode].max()\n",
    "    fourier_features = calculate_fourier_terms(\n",
    "        df[column_to_encode].astype(int).values,\n",
    "        max_cycle=max_value,\n",
    "        n_fourier_terms=n_fourier_terms,\n",
    "    )\n",
    "    feature_names = [\n",
    "        f'{column_to_encode}_sin_{i}' for i in range(1, n_fourier_terms + 1)\n",
    "    ] + [f'{column_to_encode}_cos_{i}' for i in range(1, n_fourier_terms + 1)]\n",
    "    df[feature_names] = fourier_features\n",
    "    if use_32_bit:\n",
    "        df[feature_names] = df[feature_names].astype('float32')\n",
    "    return df, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Fourier terms for all of the specified seasonal cycle columns (month, week, hour, etc.)\n",
    "# The max_values are the maximum values that season cycles can attain, e.g. for month, max_value is 12.\n",
    "def bulk_add_fourier_features(df, columns_to_encode, max_values, n_fourier_terms=1, use_32_bit=False):\n",
    "    added_features = []\n",
    "    for column_to_encode, max_value in zip(columns_to_encode, max_values):\n",
    "        df, features = add_fourier_features(\n",
    "            df,\n",
    "            column_to_encode,\n",
    "            max_value,\n",
    "            n_fourier_terms=n_fourier_terms,\n",
    "            use_32_bit=use_32_bit,\n",
    "        )\n",
    "        added_features += features\n",
    "    return df, added_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, added_features = bulk_add_fourier_features(\n",
    "    full_df,\n",
    "    ['timestamp_Month', 'timestamp_Hour', 'timestamp_Minute'],\n",
    "    max_values=[12, 24, 60],\n",
    "    n_fourier_terms=5,\n",
    "    use_32_bit=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our feature-engineered version of the dataset for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[full_df['type'] == 'train'].drop(columns='type').to_parquet(\n",
    "    preprocessed / 'selected_blocks_train_missing_imputed_feature_engg.parquet'\n",
    ")\n",
    "full_df[full_df['type'] == 'val'].drop(columns='type').to_parquet(\n",
    "    preprocessed / 'selected_blocks_val_missing_imputed_feature_engg.parquet'\n",
    ")\n",
    "full_df[full_df['type'] == 'test'].drop(columns='type').to_parquet(\n",
    "    preprocessed / 'selected_blocks_test_missing_imputed_feature_engg.parquet'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
